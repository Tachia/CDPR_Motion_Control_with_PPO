# -*- coding: utf-8 -*-
"""PPO model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/ppo-model-84f49cb9-832a-4272-8a46-56ba9b71581d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241211/auto/storage/goog4_request%26X-Goog-Date%3D20241211T005708Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D155b6a6b9b3848524f4288a4e4c7e9b0af4f1964c5d5b131c87255d17489d504dea7df7b12b3f31fdca648fb0db09e2b8a3d7ada5260054b3586d6d65b0458b22794f02ead949339c9210053b4fd73c12444d4f06e760fad252688473d6baa08f0111d2e657313483277f322f5fb1cdc068cb7eb6966b71cf012e7ce934c2105be84210734206a5b471ac46fc9a537f0af1e8b2cbcc5b26fd22383a92022b0ee3eaa3cbcaa5030da8e77bb0120122987c8eaf4e08889a8c0afa14c1a4e05d650821fab73c5b0afa9fef556da3c872a176f84f0b33ba879059209405989622195149c961c53ee20611016b8bc296468760e0436d184972dcb93152a0b77636b72
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

rogernickanaedevha_ppo_dataset_1_path = kagglehub.dataset_download('rogernickanaedevha/ppo-dataset-1')

print('Data source import complete.')

"""## intallations and importing libraries"""

!pip install pandas numpy matplotlib seaborn torch stable-baselines3 plotly gymnasium

# First, install required packages
!pip install gymnasium
!pip install stable-baselines3

"""## Importing Libraries"""

# Now import all required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import torch
import torch.nn as nn
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from scipy.interpolate import interp1d
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Wait a few seconds for installations to complete
import time
time.sleep(5)

import torch.nn.functional as F
from torch.distributions import Normal
from gymnasium import spaces

"""## CDPR Dataset"""

class CDPRDataset:
    def __init__(self, file_path):
        self.raw_data = None
        self.configurations = {
            '4-cable': None,
            '3-cable': None,
            '2-cable': None
        }

        try:
            # Load data with numeric parsing
            self.raw_data = pd.read_csv(
                file_path,
                delimiter=';',
                encoding='latin1',
                decimal=',',  # Use comma as decimal separator
                na_values=['', ' ', 'NA', 'NaN'],
                keep_default_na=True
            )
            print("Data loaded successfully")
            print(f"Shape of raw data: {self.raw_data.shape}")

            self._process_all_configurations()

        except Exception as e:
            print(f"Error loading data: {str(e)}")

    def _process_all_configurations(self):
        """Process data for all configurations"""
        try:
            # Find indices for each configuration section
            indices = {
                '4-cable': {'start': None, 'end': None},
                '3-cable': {'start': None, 'end': None},
                '2-cable': {'start': None, 'end': None}
            }

            # Find section boundaries
            for idx, row in self.raw_data.iterrows():
                if isinstance(row.iloc[0], str):
                    if '4-cable uacdpr data' in row.iloc[0]:
                        indices['4-cable']['start'] = idx + 1
                    elif '3-cable uacdpr data' in row.iloc[0]:
                        indices['4-cable']['end'] = idx
                        indices['3-cable']['start'] = idx + 1
                    elif '2-cable uacdpr data' in row.iloc[0]:
                        indices['3-cable']['end'] = idx
                        indices['2-cable']['start'] = idx + 1

            # Set end of 2-cable section
            indices['2-cable']['end'] = len(self.raw_data)

            # Process each configuration
            for config in ['4-cable', '3-cable', '2-cable']:
                start_idx = indices[config]['start']
                end_idx = indices[config]['end']

                if start_idx is not None and end_idx is not None:
                    # Extract and process data
                    data_section = self.raw_data.iloc[start_idx:end_idx].copy()

                    # Convert all columns to numeric, coercing errors to NaN
                    numeric_data = data_section.apply(pd.to_numeric, errors='coerce')

                    # Extract relevant columns
                    ee_pose = numeric_data.iloc[:, 3:9].values
                    cable_lengths = numeric_data.iloc[:, 10:14].values

                    # Remove rows with NaN values
                    mask = ~(np.isnan(ee_pose).any(axis=1) | np.isnan(cable_lengths).any(axis=1))

                    if mask.any():
                        processed_data = {
                            'ee_pose': ee_pose[mask],
                            'cable_lengths': cable_lengths[mask]
                        }

                        self.configurations[config] = processed_data
                        print(f"\nSuccessfully processed {config} data")
                        print(f"Number of valid samples: {sum(mask)}")
                        print(f"Sample ee_pose shape: {processed_data['ee_pose'].shape}")
                        print(f"Sample cable_lengths shape: {processed_data['cable_lengths'].shape}")

        except Exception as e:
            print(f"Error in processing configurations: {str(e)}")

"""## Enhanced Cable Actor-Critic Network"""

class CableActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()

        # Replace BatchNorm with LayerNorm which works better with single samples
        self.features = nn.Sequential(
            nn.LayerNorm(state_dim),
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.1),  # Add dropout for regularization
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.LayerNorm(256)
        )

        # Initialize actor components with proper scaling
        self.actor_mean = nn.Sequential(
            nn.Linear(256, action_dim),
            nn.Tanh()  # Bound the output
        )

        # Initialize log std with small negative values for stable start
        self.actor_log_std = nn.Parameter(torch.ones(action_dim) * -0.5)

        # Value network
        self.critic = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Linear(128, 1)
        )

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=0.1)
            if module.bias is not None:
                module.bias.data.zero_()

    def forward(self, state):
        features = self.features(state)

        # Generate action mean and std
        action_mean = self.actor_mean(features)
        action_std = torch.exp(self.actor_log_std).clamp(min=1e-6, max=1.0)

        # Compute value
        value = self.critic(features)

        return action_mean, action_std, value

"""## Enhanced PPO implementation"""

class ImprovedPPO:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, c1=1, c2=0.01):
        self.actor_critic = CableActorCritic(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(
            self.actor_critic.parameters(),
            lr=lr,
            eps=1e-5  # For numerical stability
        )

        self.gamma = gamma
        self.epsilon = epsilon
        self.c1 = c1
        self.c2 = c2

        # Set to evaluation mode initially
        self.actor_critic.eval()

    def get_action(self, state):
        with torch.no_grad():
            state = torch.FloatTensor(state)

            # Check for NaN values in state
            if torch.isnan(state).any():
                print("Warning: NaN values detected in state")
                state = torch.nan_to_num(state, nan=0.0)

            # Get policy distributions and value
            action_mean, action_std, _ = self.actor_critic(state)

            # Create normal distribution
            dist = Normal(action_mean, action_std)

            # Sample action and compute log probability
            action = dist.sample()
            log_prob = dist.log_prob(action).sum()

            # Clip actions to valid range
            action = torch.clamp(action, min=-0.1, max=0.1)

            return action.numpy(), log_prob.item()

"""## CDPR training System"""

class CDPRTrainer:
    def __init__(self, env, ppo_agent):
        self.env = env
        self.agent = ppo_agent
        self.training_metrics = {
            'episode_rewards': [],
            'pose_errors': [],
            'tension_violations': []
        }

        # Add experience buffer
        self.buffer_size = 1024
        self.clear_buffer()

    def clear_buffer(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []
        self.values = []

    def train(self, num_episodes):
        for episode in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            pose_errors = []
            tension_violations = []

            # Set to evaluation mode for collecting experience
            self.agent.actor_critic.eval()

            while True:
                # Get action and value
                action, log_prob = self.agent.get_action(state)
                next_state, reward, done, _, _ = self.env.step(action)

                # Store experience
                self.states.append(state)
                self.actions.append(action)
                self.rewards.append(reward)
                self.log_probs.append(log_prob)

                episode_reward += reward
                pose_errors.append(np.linalg.norm(next_state[:6] - state[:6]))
                tension_violations.append(np.sum(next_state[6:] < 0))

                if done:
                    break

                state = next_state

            # Update metrics
            self.training_metrics['episode_rewards'].append(episode_reward)
            self.training_metrics['pose_errors'].append(np.mean(pose_errors))
            self.training_metrics['tension_violations'].append(np.mean(tension_violations))

            if episode % 10 == 0:
                print(f"Episode {episode}")
                print(f"Average Reward: {episode_reward:.4f}")
                print(f"Average Pose Error: {np.mean(pose_errors):.4f}")
                print(f"Tension Violations: {np.mean(tension_violations):.4f}")
                print("-" * 50)

            # Clear buffer after episode
            self.clear_buffer()


    def plot_training_metrics(self):
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))

        ax1.plot(self.training_metrics['episode_rewards'])
        ax1.set_title('Episode Rewards')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Reward')

        ax2.plot(self.training_metrics['pose_errors'])
        ax2.set_title('Average Pose Errors')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Error (m)')

        ax3.plot(self.training_metrics['tension_violations'])
        ax3.set_title('Cable Tension Violations')
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Violations')

        plt.tight_layout()
        plt.show()

"""## CDPR Environment"""

class CDPREnvironment(gym.Env):
    def __init__(self, dataset, config='4-cable'):
        super().__init__()
        self.dataset = dataset
        self.config = config
        self.data = dataset.configurations[config]

        if self.data is None:
            raise ValueError(f"No valid data found for {config} configuration")

        # Enhanced state space including tensions
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(10,),  # 6 DOF pose + 4 cable tensions
            dtype=np.float32
        )

        # Action space for cable length adjustments
        self.action_space = spaces.Box(
            low=-0.1,
            high=0.1,
            shape=(4,),  # Cable length adjustments
            dtype=np.float32
        )

        # Compute normalization parameters
        self._compute_normalization_params()

        self.current_step = 0
        self.max_steps = len(self.data['ee_pose']) - 1
        self.reset()

    def _compute_normalization_params(self):
        """Compute normalization parameters for state space"""
        self.ee_pose_mean = np.mean(self.data['ee_pose'], axis=0)
        self.ee_pose_std = np.std(self.data['ee_pose'], axis=0) + 1e-8
        self.cable_lengths_mean = np.mean(self.data['cable_lengths'], axis=0)
        self.cable_lengths_std = np.std(self.data['cable_lengths'], axis=0) + 1e-8

    def calculate_cable_tensions(self, cable_lengths, ee_pose):
        """Calculate cable tensions using simplified dynamics model"""
        k = 1000.0  # Spring constant
        l0 = 1.0    # Rest length
        tensions = k * (cable_lengths - l0)
        return np.clip(tensions, 0, None)

    def _normalize_state(self, ee_pose, tensions):
        """Normalize state components"""
        ee_pose_norm = (ee_pose - self.ee_pose_mean) / self.ee_pose_std
        tensions_norm = tensions / 1000.0  # Simple scaling for tensions
        return np.concatenate([ee_pose_norm, tensions_norm])

    def reset(self, seed=None):
        super().reset(seed=seed)
        self.current_step = 0

        self.current_pose = self.data['ee_pose'][0].copy()
        self.current_lengths = self.data['cable_lengths'][0].copy()
        self.current_tensions = self.calculate_cable_tensions(
            self.current_lengths,
            self.current_pose
        )

        state = self._normalize_state(self.current_pose, self.current_tensions)
        return state.astype(np.float32), {}

    def step(self, action):
        # Apply action (cable length adjustments)
        new_lengths = self.current_lengths + action

        # Get target pose from dataset
        target_pose = self.data['ee_pose'][self.current_step + 1]

        # Calculate new tensions
        new_tensions = self.calculate_cable_tensions(new_lengths, target_pose)

        # Calculate rewards
        pose_error = np.linalg.norm(target_pose - self.current_pose)
        tension_penalty = np.sum(np.maximum(0, -new_tensions))
        stability_reward = -np.std(new_tensions)

        reward = -pose_error - 0.1 * tension_penalty + 0.05 * stability_reward

        # Update current state
        self.current_pose = target_pose
        self.current_lengths = new_lengths
        self.current_tensions = new_tensions
        self.current_step += 1

        # Create normalized state
        state = self._normalize_state(self.current_pose, self.current_tensions)

        # Check if episode is done
        done = self.current_step >= self.max_steps

        return state.astype(np.float32), reward, done, False, {}

"""# Main implementation"""

def main():
    # Initialize dataset
    dataset = CDPRDataset('/kaggle/input/ppo-dataset-1/final_experiment_set copy.csv')

    # Create environment
    env = CDPREnvironment(dataset)

    # Initialize PPO agent
    state_dim = 10  # 6 DOF pose + 4 cable tensions
    action_dim = 4  # 4 cable length adjustments
    ppo_agent = ImprovedPPO(state_dim, action_dim)

    # Create trainer
    trainer = CDPRTrainer(env, ppo_agent)

    # Train the agent
    trainer.train(num_episodes=100)

    # Plot training results
    trainer.plot_training_metrics()

if __name__ == "__main__":
    main()